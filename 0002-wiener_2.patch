From 269eeaf7c01afc79a53537881ad03185bf491cf6 Mon Sep 17 00:00:00 2001
From: "Nathan E. Egge" <unlord@xiph.org>
Date: Tue, 29 Dec 2020 06:58:33 -0500
Subject: [PATCH] Add bpc suffix to lr functions

---
 src/x86/looprestoration.asm         |  36 ++---
 src/x86/looprestoration_init_tmpl.c | 204 +++++++++++++---------------
 src/x86/looprestoration_sse.asm     |  60 ++++----
 3 files changed, 146 insertions(+), 154 deletions(-)

diff --git a/src/x86/looprestoration.asm b/src/x86/looprestoration.asm
index 8ebe230..e077cdd 100644
--- a/src/x86/looprestoration.asm
+++ b/src/x86/looprestoration.asm
@@ -66,8 +66,8 @@ SECTION .text
 DECLARE_REG_TMP 4, 9, 7, 11, 12, 13, 14 ; wiener ring buffer pointers
 
 INIT_YMM avx2
-cglobal wiener_filter7, 5, 15, 16, -384*12-16, dst, dst_stride, left, lpf, \
-                                               lpf_stride, w, edge, flt, h
+cglobal wiener_filter7_8bpc, 5, 15, 16, -384*12-16, dst, dst_stride, left, lpf, \
+                                                    lpf_stride, w, edge, flt, h
     mov           fltq, fltmp
     mov          edged, r8m
     mov             wd, wm
@@ -414,8 +414,8 @@ ALIGN function_align
     add           dstq, dst_strideq
     ret
 
-cglobal wiener_filter5, 5, 13, 16, 384*8+16, dst, dst_stride, left, lpf, \
-                                             lpf_stride, w, edge, flt, h
+cglobal wiener_filter5_8bpc, 5, 13, 16, 384*8+16, dst, dst_stride, left, lpf, \
+                                                  lpf_stride, w, edge, flt, h
     mov           fltq, fltmp
     mov          edged, r8m
     mov             wd, wm
@@ -532,7 +532,7 @@ cglobal wiener_filter5, 5, 13, 16, 384*8+16, dst, dst_stride, left, lpf, \
     jnz .h_have_right
     cmp           r10d, -33
     jl .h_have_right
-    call mangle(private_prefix %+ _wiener_filter7_avx2).extend_right
+    call mangle(private_prefix %+ _wiener_filter7_8bpc_avx2).extend_right
 .h_have_right:
     pshufb          m0, m4, m6
     pmaddubsw       m0, m12
@@ -591,7 +591,7 @@ ALIGN function_align
     jnz .hv_have_right
     cmp           r10d, -33
     jl .hv_have_right
-    call mangle(private_prefix %+ _wiener_filter7_avx2).extend_right
+    call mangle(private_prefix %+ _wiener_filter7_8bpc_avx2).extend_right
 .hv_have_right:
     pshufb          m0, m4, m6
     pmaddubsw       m0, m12
@@ -705,7 +705,7 @@ ALIGN function_align
     jl .v_loop
     ret
 
-cglobal sgr_box3_h, 5, 11, 7, sumsq, sum, left, src, stride, w, h, edge, x, xlim
+cglobal sgr_box3_h_8bpc, 5, 11, 7, sumsq, sum, left, src, stride, w, h, edge, x, xlim
     mov        xlimd, edgem
     movifnidn     wd, wm
     mov           hd, hm
@@ -805,7 +805,7 @@ cglobal sgr_box3_h, 5, 11, 7, sumsq, sum, left, src, stride, w, h, edge, x, xlim
     RET
 
 INIT_YMM avx2
-cglobal sgr_box3_v, 4, 10, 9, sumsq, sum, w, h, edge, x, y, sumsq_ptr, sum_ptr, ylim
+cglobal sgr_box3_v_8bpc, 4, 10, 9, sumsq, sum, w, h, edge, x, y, sumsq_ptr, sum_ptr, ylim
     movifnidn  edged, edgem
     mov           xq, -2
     rorx       ylimd, edged, 2
@@ -868,7 +868,7 @@ cglobal sgr_box3_v, 4, 10, 9, sumsq, sum, w, h, edge, x, y, sumsq_ptr, sum_ptr,
     RET
 
 INIT_YMM avx2
-cglobal sgr_calc_ab1, 4, 6, 11, a, b, w, h, s
+cglobal sgr_calc_ab1_8bpc, 4, 6, 11, a, b, w, h, s
     sub           aq, (384+16-1)*4
     sub           bq, (384+16-1)*2
     add           hd, 2
@@ -937,8 +937,8 @@ cglobal sgr_calc_ab1, 4, 6, 11, a, b, w, h, s
     RET
 
 INIT_YMM avx2
-cglobal sgr_finish_filter1, 5, 13, 16, t, src, stride, a, b, w, h, \
-                                       tmp_ptr, src_ptr, a_ptr, b_ptr, x, y
+cglobal sgr_finish_filter1_8bpc, 5, 13, 16, t, src, stride, a, b, w, h, \
+                                            tmp_ptr, src_ptr, a_ptr, b_ptr, x, y
     movifnidn     wd, wm
     mov           hd, hm
     vpbroadcastd m15, [pw_16]
@@ -1043,7 +1043,7 @@ cglobal sgr_finish_filter1, 5, 13, 16, t, src, stride, a, b, w, h, \
     RET
 
 INIT_YMM avx2
-cglobal sgr_weighted1, 4, 6, 6, dst, stride, t, w, h, wt
+cglobal sgr_weighted1_8bpc, 4, 6, 6, dst, stride, t, w, h, wt
 %ifidn wtd, wtm
     shl          wtd, 4
     movd         xm5, wtd
@@ -1082,7 +1082,7 @@ cglobal sgr_weighted1, 4, 6, 6, dst, stride, t, w, h, wt
     RET
 
 INIT_YMM avx2
-cglobal sgr_box5_h, 5, 11, 10, sumsq, sum, left, src, stride, w, h, edge, x, xlim
+cglobal sgr_box5_h_8bpc, 5, 11, 10, sumsq, sum, left, src, stride, w, h, edge, x, xlim
     mov        edged, edgem
     movifnidn     wd, wm
     mov           hd, hm
@@ -1200,7 +1200,7 @@ cglobal sgr_box5_h, 5, 11, 10, sumsq, sum, left, src, stride, w, h, edge, x, xli
     RET
 
 INIT_YMM avx2
-cglobal sgr_box5_v, 4, 10, 15, sumsq, sum, w, h, edge, x, y, sumsq_ptr, sum_ptr, ylim
+cglobal sgr_box5_v_8bpc, 4, 10, 15, sumsq, sum, w, h, edge, x, y, sumsq_ptr, sum_ptr, ylim
     movifnidn  edged, edgem
     mov           xq, -2
     rorx       ylimd, edged, 2
@@ -1293,7 +1293,7 @@ cglobal sgr_box5_v, 4, 10, 15, sumsq, sum, w, h, edge, x, y, sumsq_ptr, sum_ptr,
     jmp .loop_y_noload
 
 INIT_YMM avx2
-cglobal sgr_calc_ab2, 4, 6, 11, a, b, w, h, s
+cglobal sgr_calc_ab2_8bpc, 4, 6, 11, a, b, w, h, s
     sub           aq, (384+16-1)*4
     sub           bq, (384+16-1)*2
     add           hd, 2
@@ -1364,8 +1364,8 @@ cglobal sgr_calc_ab2, 4, 6, 11, a, b, w, h, s
     RET
 
 INIT_YMM avx2
-cglobal sgr_finish_filter2, 5, 13, 13, t, src, stride, a, b, w, h, \
-                                       tmp_ptr, src_ptr, a_ptr, b_ptr, x, y
+cglobal sgr_finish_filter2_8bpc, 5, 13, 13, t, src, stride, a, b, w, h, \
+                                            tmp_ptr, src_ptr, a_ptr, b_ptr, x, y
     movifnidn     wd, wm
     mov           hd, hm
     vpbroadcastd  m9, [pw_5_6]
@@ -1483,7 +1483,7 @@ cglobal sgr_finish_filter2, 5, 13, 13, t, src, stride, a, b, w, h, \
     RET
 
 INIT_YMM avx2
-cglobal sgr_weighted2, 4, 7, 11, dst, stride, t1, t2, w, h, wt
+cglobal sgr_weighted2_8bpc, 4, 7, 11, dst, stride, t1, t2, w, h, wt
     movifnidn     wd, wm
     movifnidn     hd, hm
     vpbroadcastd  m0, wtm
diff --git a/src/x86/looprestoration_init_tmpl.c b/src/x86/looprestoration_init_tmpl.c
index 5df449c..11ebdd1 100644
--- a/src/x86/looprestoration_init_tmpl.c
+++ b/src/x86/looprestoration_init_tmpl.c
@@ -31,148 +31,140 @@
 #include "common/intops.h"
 #include "src/tables.h"
 
-#define WIENER_FILTER(ext) \
-void dav1d_wiener_filter7_##ext(pixel *const dst, ptrdiff_t dst_stride, \
-                                const pixel (*left)[4], const pixel *lpf, \
-                                ptrdiff_t lpf_stride, int w, int h, \
-                                const int16_t filter[2][8], \
-                                enum LrEdgeFlags edges); \
-void dav1d_wiener_filter5_##ext(pixel *const dst, ptrdiff_t dst_stride, \
-                                const pixel (*left)[4], const pixel *lpf, \
-                                ptrdiff_t lpf_stride, int w, int h, \
-                                const int16_t filter[2][8], \
-                                enum LrEdgeFlags edges);
+#define decl_wiener_filter_fns(ext) \
+decl_wiener_filter_fn(BF(dav1d_wiener_filter7, ext)); \
+decl_wiener_filter_fn(BF(dav1d_wiener_filter5, ext))
 
-#define SGR_FILTER(ext) \
-void dav1d_sgr_box3_h_##ext(int32_t *sumsq, int16_t *sum, \
-                            const pixel (*left)[4], \
-                            const pixel *src, const ptrdiff_t stride, \
-                            const int w, const int h, \
-                            const enum LrEdgeFlags edges); \
-void dav1d_sgr_box3_v_##ext(int32_t *sumsq, int16_t *sum, \
-                            const int w, const int h, \
-                            const enum LrEdgeFlags edges); \
-void dav1d_sgr_calc_ab1_##ext(int32_t *a, int16_t *b, \
-                              const int w, const int h, const int strength); \
-void dav1d_sgr_finish_filter1_##ext(coef *tmp, \
-                                    const pixel *src, const ptrdiff_t stride, \
-                                    const int32_t *a, const int16_t *b, \
-                                    const int w, const int h); \
+#define decl_sgr_filter_fn(ext) \
+void BF(dav1d_sgr_box3_h, ext)(int32_t *sumsq, int16_t *sum, \
+                               const pixel (*left)[4], \
+                               const pixel *src, const ptrdiff_t stride, \
+                               const int w, const int h, \
+                               const enum LrEdgeFlags edges); \
+void BF(dav1d_sgr_box3_v, ext)(int32_t *sumsq, int16_t *sum, \
+                               const int w, const int h, \
+                               const enum LrEdgeFlags edges); \
+void BF(dav1d_sgr_calc_ab1, ext)(int32_t *a, int16_t *b, \
+                                 const int w, const int h, const int strength); \
+void BF(dav1d_sgr_finish_filter1, ext)(coef *tmp, \
+                                       const pixel *src, const ptrdiff_t stride, \
+                                       const int32_t *a, const int16_t *b, \
+                                       const int w, const int h); \
 \
 /* filter with a 3x3 box (radius=1) */ \
-static void dav1d_sgr_filter1_##ext(coef *tmp, \
-                                    const pixel *src, const ptrdiff_t stride, \
-                                    const pixel (*left)[4], \
-                                    const pixel *lpf, const ptrdiff_t lpf_stride, \
-                                    const int w, const int h, const int strength, \
-                                    const enum LrEdgeFlags edges) \
+static void BF(dav1d_sgr_filter1, ext)(coef *tmp, \
+                                       const pixel *src, const ptrdiff_t stride, \
+                                       const pixel (*left)[4], \
+                                       const pixel *lpf, const ptrdiff_t lpf_stride, \
+                                       const int w, const int h, const int strength, \
+                                       const enum LrEdgeFlags edges) \
 { \
     ALIGN_STK_32(int32_t, sumsq_mem, (384 + 16) * 68 + 8,); \
     int32_t *const sumsq = &sumsq_mem[(384 + 16) * 2 + 8], *const a = sumsq; \
     ALIGN_STK_32(int16_t, sum_mem, (384 + 16) * 68 + 16,); \
     int16_t *const sum = &sum_mem[(384 + 16) * 2 + 16], *const b = sum; \
 \
-    dav1d_sgr_box3_h_##ext(sumsq, sum, left, src, stride, w, h, edges); \
+    BF(dav1d_sgr_box3_h, ext)(sumsq, sum, left, src, stride, w, h, edges); \
     if (edges & LR_HAVE_TOP) \
-        dav1d_sgr_box3_h_##ext(&sumsq[-2 * (384 + 16)], &sum[-2 * (384 + 16)], \
-                              NULL, lpf, lpf_stride, w, 2, edges); \
+        BF(dav1d_sgr_box3_h, ext)(&sumsq[-2 * (384 + 16)], &sum[-2 * (384 + 16)], \
+                                  NULL, lpf, lpf_stride, w, 2, edges); \
 \
     if (edges & LR_HAVE_BOTTOM) \
-        dav1d_sgr_box3_h_##ext(&sumsq[h * (384 + 16)], &sum[h * (384 + 16)], \
-                              NULL, lpf + 6 * PXSTRIDE(lpf_stride), \
-                              lpf_stride, w, 2, edges); \
+        BF(dav1d_sgr_box3_h, ext)(&sumsq[h * (384 + 16)], &sum[h * (384 + 16)], \
+                                  NULL, lpf + 6 * PXSTRIDE(lpf_stride), \
+                                  lpf_stride, w, 2, edges); \
 \
-    dav1d_sgr_box3_v_##ext(sumsq, sum, w, h, edges); \
-    dav1d_sgr_calc_ab1_##ext(a, b, w, h, strength); \
-    dav1d_sgr_finish_filter1_##ext(tmp, src, stride, a, b, w, h); \
+    BF(dav1d_sgr_box3_v, ext)(sumsq, sum, w, h, edges); \
+    BF(dav1d_sgr_calc_ab1, ext)(a, b, w, h, strength); \
+    BF(dav1d_sgr_finish_filter1, ext)(tmp, src, stride, a, b, w, h); \
 } \
 \
-void dav1d_sgr_box5_h_##ext(int32_t *sumsq, int16_t *sum, \
-                            const pixel (*left)[4], \
-                            const pixel *src, const ptrdiff_t stride, \
-                            const int w, const int h, \
-                            const enum LrEdgeFlags edges); \
-void dav1d_sgr_box5_v_##ext(int32_t *sumsq, int16_t *sum, \
-                            const int w, const int h, \
-                            const enum LrEdgeFlags edges); \
-void dav1d_sgr_calc_ab2_##ext(int32_t *a, int16_t *b, \
-                              const int w, const int h, const int strength); \
-void dav1d_sgr_finish_filter2_##ext(coef *tmp, \
-                                    const pixel *src, const ptrdiff_t stride, \
-                                    const int32_t *a, const int16_t *b, \
-                                    const int w, const int h); \
+void BF(dav1d_sgr_box5_h, ext)(int32_t *sumsq, int16_t *sum, \
+                               const pixel (*left)[4], \
+                               const pixel *src, const ptrdiff_t stride, \
+                               const int w, const int h, \
+                               const enum LrEdgeFlags edges); \
+void BF(dav1d_sgr_box5_v, ext)(int32_t *sumsq, int16_t *sum, \
+                               const int w, const int h, \
+                               const enum LrEdgeFlags edges); \
+void BF(dav1d_sgr_calc_ab2, ext)(int32_t *a, int16_t *b, \
+                                 const int w, const int h, const int strength); \
+void BF(dav1d_sgr_finish_filter2, ext)(coef *tmp, \
+                                       const pixel *src, const ptrdiff_t stride, \
+                                       const int32_t *a, const int16_t *b, \
+                                       const int w, const int h); \
 \
 /* filter with a 5x5 box (radius=2) */ \
-static void dav1d_sgr_filter2_##ext(coef *tmp, \
-                                    const pixel *src, const ptrdiff_t stride, \
-                                    const pixel (*left)[4], \
-                                    const pixel *lpf, const ptrdiff_t lpf_stride, \
-                                    const int w, const int h, const int strength, \
-                                    const enum LrEdgeFlags edges) \
+static void BF(dav1d_sgr_filter2, ext)(coef *tmp, \
+                                       const pixel *src, const ptrdiff_t stride, \
+                                       const pixel (*left)[4], \
+                                       const pixel *lpf, const ptrdiff_t lpf_stride, \
+                                       const int w, const int h, const int strength, \
+                                       const enum LrEdgeFlags edges) \
 { \
     ALIGN_STK_32(int32_t, sumsq_mem, (384 + 16) * 68 + 8,); \
     int32_t *const sumsq = &sumsq_mem[(384 + 16) * 2 + 8], *const a = sumsq; \
     ALIGN_STK_32(int16_t, sum_mem, (384 + 16) * 68 + 16,); \
     int16_t *const sum = &sum_mem[(384 + 16) * 2 + 16], *const b = sum; \
 \
-    dav1d_sgr_box5_h_##ext(sumsq, sum, left, src, stride, w, h, edges); \
+    BF(dav1d_sgr_box5_h, ext)(sumsq, sum, left, src, stride, w, h, edges); \
     if (edges & LR_HAVE_TOP) \
-        dav1d_sgr_box5_h_##ext(&sumsq[-2 * (384 + 16)], &sum[-2 * (384 + 16)], \
-                              NULL, lpf, lpf_stride, w, 2, edges); \
+        BF(dav1d_sgr_box5_h, ext)(&sumsq[-2 * (384 + 16)], &sum[-2 * (384 + 16)], \
+                                  NULL, lpf, lpf_stride, w, 2, edges); \
 \
     if (edges & LR_HAVE_BOTTOM) \
-        dav1d_sgr_box5_h_##ext(&sumsq[h * (384 + 16)], &sum[h * (384 + 16)], \
-                              NULL, lpf + 6 * PXSTRIDE(lpf_stride), \
-                              lpf_stride, w, 2, edges); \
+        BF(dav1d_sgr_box5_h, ext)(&sumsq[h * (384 + 16)], &sum[h * (384 + 16)], \
+                                  NULL, lpf + 6 * PXSTRIDE(lpf_stride), \
+                                  lpf_stride, w, 2, edges); \
 \
-    dav1d_sgr_box5_v_##ext(sumsq, sum, w, h, edges); \
-    dav1d_sgr_calc_ab2_##ext(a, b, w, h, strength); \
-    dav1d_sgr_finish_filter2_##ext(tmp, src, stride, a, b, w, h); \
+    BF(dav1d_sgr_box5_v, ext)(sumsq, sum, w, h, edges); \
+    BF(dav1d_sgr_calc_ab2, ext)(a, b, w, h, strength); \
+    BF(dav1d_sgr_finish_filter2, ext)(tmp, src, stride, a, b, w, h); \
 } \
 \
-void dav1d_sgr_weighted1_##ext(pixel *dst, const ptrdiff_t stride, \
-                               const coef *t1, const int w, const int h, \
-                               const int wt); \
-void dav1d_sgr_weighted2_##ext(pixel *dst, const ptrdiff_t stride, \
-                               const coef *t1, const coef *t2, \
-                               const int w, const int h, \
-                               const uint32_t wt); \
+void BF(dav1d_sgr_weighted1, ext)(pixel *dst, const ptrdiff_t stride, \
+                                  const coef *t1, const int w, const int h, \
+                                  const int wt); \
+void BF(dav1d_sgr_weighted2, ext)(pixel *dst, const ptrdiff_t stride, \
+                                  const coef *t1, const coef *t2, \
+                                  const int w, const int h, \
+                                  const uint32_t wt); \
 \
-static void sgr_filter_##ext(pixel *const dst, const ptrdiff_t dst_stride, \
-                             const pixel (*const left)[4], \
-                             const pixel *lpf, const ptrdiff_t lpf_stride, \
-                             const int w, const int h, const int sgr_idx, \
-                             const int16_t sgr_wt[7], const enum LrEdgeFlags edges) \
+static void BF(sgr_filter, ext)(pixel *const dst, const ptrdiff_t dst_stride, \
+                                const pixel (*const left)[4], \
+                                const pixel *lpf, const ptrdiff_t lpf_stride, \
+                                const int w, const int h, const int sgr_idx, \
+                                const int16_t sgr_wt[7], const enum LrEdgeFlags edges) \
 { \
     if (!dav1d_sgr_params[sgr_idx][0]) { \
         ALIGN_STK_32(coef, tmp, 64 * 384,); \
-        dav1d_sgr_filter1_##ext(tmp, dst, dst_stride, left, lpf, lpf_stride, \
-                               w, h, dav1d_sgr_params[sgr_idx][3], edges); \
-        dav1d_sgr_weighted1_##ext(dst, dst_stride, tmp, w, h, (1 << 7) - sgr_wt[1]); \
+        BF(dav1d_sgr_filter1, ext)(tmp, dst, dst_stride, left, lpf, lpf_stride, \
+                                   w, h, dav1d_sgr_params[sgr_idx][3], edges); \
+        BF(dav1d_sgr_weighted1, ext)(dst, dst_stride, tmp, w, h, (1 << 7) - sgr_wt[1]); \
     } else if (!dav1d_sgr_params[sgr_idx][1]) { \
         ALIGN_STK_32(coef, tmp, 64 * 384,); \
-        dav1d_sgr_filter2_##ext(tmp, dst, dst_stride, left, lpf, lpf_stride, \
-                               w, h, dav1d_sgr_params[sgr_idx][2], edges); \
-        dav1d_sgr_weighted1_##ext(dst, dst_stride, tmp, w, h, sgr_wt[0]); \
+        BF(dav1d_sgr_filter2, ext)(tmp, dst, dst_stride, left, lpf, lpf_stride, \
+                                   w, h, dav1d_sgr_params[sgr_idx][2], edges); \
+        BF(dav1d_sgr_weighted1, ext)(dst, dst_stride, tmp, w, h, sgr_wt[0]); \
     } else { \
         ALIGN_STK_32(coef, tmp1, 64 * 384,); \
         ALIGN_STK_32(coef, tmp2, 64 * 384,); \
-        dav1d_sgr_filter2_##ext(tmp1, dst, dst_stride, left, lpf, lpf_stride, \
-                               w, h, dav1d_sgr_params[sgr_idx][2], edges); \
-        dav1d_sgr_filter1_##ext(tmp2, dst, dst_stride, left, lpf, lpf_stride, \
-                               w, h, dav1d_sgr_params[sgr_idx][3], edges); \
+        BF(dav1d_sgr_filter2, ext)(tmp1, dst, dst_stride, left, lpf, lpf_stride, \
+                                   w, h, dav1d_sgr_params[sgr_idx][2], edges); \
+        BF(dav1d_sgr_filter1, ext)(tmp2, dst, dst_stride, left, lpf, lpf_stride, \
+                                   w, h, dav1d_sgr_params[sgr_idx][3], edges); \
         const uint32_t wt = ((128 - sgr_wt[0] - sgr_wt[1]) << 16) | (uint16_t) sgr_wt[0]; \
-        dav1d_sgr_weighted2_##ext(dst, dst_stride, tmp1, tmp2, w, h, wt); \
+        BF(dav1d_sgr_weighted2, ext)(dst, dst_stride, tmp1, tmp2, w, h, wt); \
     } \
 }
 
 #if BITDEPTH == 8
-WIENER_FILTER(sse2)
-WIENER_FILTER(ssse3)
-SGR_FILTER(ssse3)
+decl_wiener_filter_fns(sse2);
+decl_wiener_filter_fns(ssse3);
+decl_sgr_filter_fn(ssse3)
 # if ARCH_X86_64
-WIENER_FILTER(avx2)
-SGR_FILTER(avx2)
+decl_wiener_filter_fns(avx2);
+decl_sgr_filter_fn(avx2)
 # endif
 #endif
 
@@ -181,21 +173,21 @@ COLD void bitfn(dav1d_loop_restoration_dsp_init_x86)(Dav1dLoopRestorationDSPCont
 
     if (!(flags & DAV1D_X86_CPU_FLAG_SSE2)) return;
 #if BITDEPTH == 8
-    c->wiener[0] = dav1d_wiener_filter7_sse2;
-    c->wiener[1] = dav1d_wiener_filter5_sse2;
+    c->wiener[0] = BF(dav1d_wiener_filter7, sse2);
+    c->wiener[1] = BF(dav1d_wiener_filter5, sse2);
 #endif
 
     if (!(flags & DAV1D_X86_CPU_FLAG_SSSE3)) return;
 #if BITDEPTH == 8
-    c->wiener[0] = dav1d_wiener_filter7_ssse3;
-    c->wiener[1] = dav1d_wiener_filter5_ssse3;
-    c->selfguided = sgr_filter_ssse3;
+    c->wiener[0] = BF(dav1d_wiener_filter7, ssse3);
+    c->wiener[1] = BF(dav1d_wiener_filter5, ssse3);
+    c->selfguided = BF(sgr_filter, ssse3);
 #endif
 
     if (!(flags & DAV1D_X86_CPU_FLAG_AVX2)) return;
 #if BITDEPTH == 8 && ARCH_X86_64
-    c->wiener[0] = dav1d_wiener_filter7_avx2;
-    c->wiener[1] = dav1d_wiener_filter5_avx2;
-    c->selfguided = sgr_filter_avx2;
+    c->wiener[0] = BF(dav1d_wiener_filter7, avx2);
+    c->wiener[1] = BF(dav1d_wiener_filter5, avx2);
+    c->selfguided = BF(sgr_filter, avx2);
 #endif
 }
diff --git a/src/x86/looprestoration_sse.asm b/src/x86/looprestoration_sse.asm
index 5d3ca49..4b77138 100644
--- a/src/x86/looprestoration_sse.asm
+++ b/src/x86/looprestoration_sse.asm
@@ -97,8 +97,8 @@ SECTION .text
 %macro WIENER 0
 %if ARCH_X86_64
 DECLARE_REG_TMP 4, 10, 7, 11, 12, 13, 14 ; ring buffer pointers
-cglobal wiener_filter7, 5, 15, 16, -384*12-16, dst, dst_stride, left, lpf, \
-                                               lpf_stride, w, edge, flt, h, x
+cglobal wiener_filter7_8bpc, 5, 15, 16, -384*12-16, dst, dst_stride, left, lpf, \
+                                                    lpf_stride, w, edge, flt, h, x
     %define base 0
     mov           fltq, fltmp
     mov          edged, r8m
@@ -139,7 +139,7 @@ DECLARE_REG_TMP 4, 0, _, 5
     %define m11         [stk+96]
     %define stk_off     112
 %endif
-cglobal wiener_filter7, 0, 7, 8, -384*12-stk_off, _, x, left, lpf, lpf_stride
+cglobal wiener_filter7_8bpc, 0, 7, 8, -384*12-stk_off, _, x, left, lpf, lpf_stride
     %define base        r6-pb_right_ext_mask-21
     %define stk         esp
     %define dstq        leftq
@@ -245,7 +245,7 @@ cglobal wiener_filter7, 0, 7, 8, -384*12-stk_off, _, x, left, lpf, lpf_stride
     add           lpfq, [rsp+gprsize*1]
     call .hv_bottom
 .v1:
-    call mangle(private_prefix %+ _wiener_filter7_ssse3).v
+    call mangle(private_prefix %+ _wiener_filter7_8bpc_ssse3).v
     RET
 .no_top:
     lea             t3, [lpfq+lpf_strideq*4]
@@ -281,9 +281,9 @@ cglobal wiener_filter7, 0, 7, 8, -384*12-stk_off, _, x, left, lpf, lpf_stride
     dec             hd
     jnz .main
 .v3:
-    call mangle(private_prefix %+ _wiener_filter7_ssse3).v
+    call mangle(private_prefix %+ _wiener_filter7_8bpc_ssse3).v
 .v2:
-    call mangle(private_prefix %+ _wiener_filter7_ssse3).v
+    call mangle(private_prefix %+ _wiener_filter7_8bpc_ssse3).v
     jmp .v1
 .extend_right:
     movd            m2, [lpfq-4]
@@ -685,8 +685,8 @@ ALIGN function_align
 %endif
 
 %if ARCH_X86_64
-cglobal wiener_filter5, 5, 13, 16, 384*8+16, dst, dst_stride, left, lpf, \
-                                             lpf_stride, w, edge, flt, h, x
+cglobal wiener_filter5_8bpc, 5, 13, 16, 384*8+16, dst, dst_stride, left, lpf, \
+                                                  lpf_stride, w, edge, flt, h, x
     mov           fltq, fltmp
     mov          edged, r8m
     mov             wd, wm
@@ -720,7 +720,7 @@ cglobal wiener_filter5, 5, 13, 16, 384*8+16, dst, dst_stride, left, lpf, \
     %define m11         [stk+80]
     %define stk_off     96
 %endif
-cglobal wiener_filter5, 0, 7, 8, -384*8-stk_off, _, x, left, lpf, lpf_stride
+cglobal wiener_filter5_8bpc, 0, 7, 8, -384*8-stk_off, _, x, left, lpf, lpf_stride
     %define stk         esp
     %define leftmp      [stk+28]
     %define m8          [base+pw_m16380]
@@ -827,14 +827,14 @@ cglobal wiener_filter5, 0, 7, 8, -384*8-stk_off, _, x, left, lpf, lpf_stride
     dec             hd
     jnz .main
 .v2:
-    call mangle(private_prefix %+ _wiener_filter5_ssse3).v
+    call mangle(private_prefix %+ _wiener_filter5_8bpc_ssse3).v
     add           dstq, dst_strideq
     mov             t4, t3
     mov             t3, t2
     mov             t2, t1
     movifnidn    dstmp, dstq
 .v1:
-    call mangle(private_prefix %+ _wiener_filter5_ssse3).v
+    call mangle(private_prefix %+ _wiener_filter5_8bpc_ssse3).v
     jmp .end
 .h:
     %define stk esp+4
@@ -873,7 +873,7 @@ cglobal wiener_filter5, 0, 7, 8, -384*8-stk_off, _, x, left, lpf, lpf_stride
     jnz .h_have_right
     cmp             xd, -17
     jl .h_have_right
-    call mangle(private_prefix %+ _wiener_filter7 %+ SUFFIX).extend_right
+    call mangle(private_prefix %+ _wiener_filter7_8bpc %+ SUFFIX).extend_right
 .h_have_right:
 %macro %%h5 0
 %if cpuflag(ssse3)
@@ -991,7 +991,7 @@ ALIGN function_align
     jnz .hv_have_right
     cmp             xd, -17
     jl .hv_have_right
-    call mangle(private_prefix %+ _wiener_filter7 %+ SUFFIX).extend_right
+    call mangle(private_prefix %+ _wiener_filter7_8bpc %+ SUFFIX).extend_right
 .hv_have_right:
     %%h5
     mova            m2, [t3+xq*2]
@@ -1161,7 +1161,7 @@ WIENER
 %endmacro
 
 %if ARCH_X86_64
-cglobal sgr_box3_h, 5, 11, 8, sumsq, sum, left, src, stride, x, h, edge, w, xlim
+cglobal sgr_box3_h_8bpc, 5, 11, 8, sumsq, sum, left, src, stride, x, h, edge, w, xlim
     mov        xlimd, edgem
     movifnidn     xd, xm
     mov           hd, hm
@@ -1170,7 +1170,7 @@ cglobal sgr_box3_h, 5, 11, 8, sumsq, sum, left, src, stride, x, h, edge, w, xlim
     add           xd, xlimd
     xor        xlimd, 2                             ; 2*!have_right
 %else
-cglobal sgr_box3_h, 6, 7, 8, sumsq, sum, left, src, stride, x, h, edge, w, xlim
+cglobal sgr_box3_h_8bpc, 6, 7, 8, sumsq, sum, left, src, stride, x, h, edge, w, xlim
  %define wq     r0m
  %define xlimd  r1m
  %define hd     hmp
@@ -1287,10 +1287,10 @@ cglobal sgr_box3_h, 6, 7, 8, sumsq, sum, left, src, stride, x, h, edge, w, xlim
     RET
 
 %if ARCH_X86_64
-cglobal sgr_box3_v, 4, 10, 9, sumsq, sum, w, h, edge, x, y, sumsq_base, sum_base, ylim
+cglobal sgr_box3_v_8bpc, 4, 10, 9, sumsq, sum, w, h, edge, x, y, sumsq_base, sum_base, ylim
     movifnidn  edged, edgem
 %else
-cglobal sgr_box3_v, 3, 7, 8, -28, sumsq, sum, w, edge, h, x, y
+cglobal sgr_box3_v_8bpc, 3, 7, 8, -28, sumsq, sum, w, edge, h, x, y
  %define sumsq_baseq dword [esp+0]
  %define sum_baseq   dword [esp+4]
  %define ylimd       dword [esp+8]
@@ -1383,7 +1383,7 @@ cglobal sgr_box3_v, 3, 7, 8, -28, sumsq, sum, w, edge, h, x, y
     jl .loop_x
     RET
 
-cglobal sgr_calc_ab1, 4, 7, 12, a, b, w, h, s
+cglobal sgr_calc_ab1_8bpc, 4, 7, 12, a, b, w, h, s
     movifnidn     sd, sm
     sub           aq, (384+16-1)*4
     sub           bq, (384+16-1)*2
@@ -1463,8 +1463,8 @@ cglobal sgr_calc_ab1, 4, 7, 12, a, b, w, h, s
     RET
 
 %if ARCH_X86_64
-cglobal sgr_finish_filter1, 5, 13, 16, t, src, stride, a, b, w, h, \
-                                       tmp_base, src_base, a_base, b_base, x, y
+cglobal sgr_finish_filter1_8bpc, 5, 13, 16, t, src, stride, a, b, w, h, \
+                                            tmp_base, src_base, a_base, b_base, x, y
     movifnidn     wd, wm
     mov           hd, hm
     mova         m15, [pw_16]
@@ -1474,7 +1474,7 @@ cglobal sgr_finish_filter1, 5, 13, 16, t, src, stride, a, b, w, h, \
     mov      b_baseq, bq
     xor           xd, xd
 %else
-cglobal sgr_finish_filter1, 7, 7, 8, -144, t, src, stride, a, b, x, y
+cglobal sgr_finish_filter1_8bpc, 7, 7, 8, -144, t, src, stride, a, b, x, y
  %define tmp_baseq  [esp+8]
  %define src_baseq  [esp+12]
  %define a_baseq    [esp+16]
@@ -1688,7 +1688,7 @@ cglobal sgr_finish_filter1, 7, 7, 8, -144, t, src, stride, a, b, x, y
     jl .loop_x
     RET
 
-cglobal sgr_weighted1, 4, 7, 8, dst, stride, t, w, h, wt
+cglobal sgr_weighted1_8bpc, 4, 7, 8, dst, stride, t, w, h, wt
     movifnidn     hd, hm
 %if ARCH_X86_32
     SETUP_PIC r6, 0
@@ -1726,14 +1726,14 @@ cglobal sgr_weighted1, 4, 7, 8, dst, stride, t, w, h, wt
     RET
 
 %if ARCH_X86_64
-cglobal sgr_box5_h, 5, 11, 12, sumsq, sum, left, src, stride, w, h, edge, x, xlim
+cglobal sgr_box5_h_8bpc, 5, 11, 12, sumsq, sum, left, src, stride, w, h, edge, x, xlim
     mov        edged, edgem
     movifnidn     wd, wm
     mov           hd, hm
     mova         m10, [pb_0]
     mova         m11, [pb_0_1]
 %else
-cglobal sgr_box5_h, 7, 7, 8, sumsq, sum, left, src, xlim, x, h, edge
+cglobal sgr_box5_h_8bpc, 7, 7, 8, sumsq, sum, left, src, xlim, x, h, edge
  %define edgeb      byte edgem
  %define wd         xd
  %define wq         wd
@@ -1909,11 +1909,11 @@ cglobal sgr_box5_h, 7, 7, 8, sumsq, sum, left, src, xlim, x, h, edge
     RET
 
 %if ARCH_X86_64
-cglobal sgr_box5_v, 4, 10, 15, sumsq, sum, w, h, edge, x, y, sumsq_ptr, sum_ptr, ylim
+cglobal sgr_box5_v_8bpc, 4, 10, 15, sumsq, sum, w, h, edge, x, y, sumsq_ptr, sum_ptr, ylim
     movifnidn  edged, edgem
     mov        ylimd, edged
 %else
-cglobal sgr_box5_v, 5, 7, 8, -44, sumsq, sum, x, y, ylim, sumsq_ptr, sum_ptr
+cglobal sgr_box5_v_8bpc, 5, 7, 8, -44, sumsq, sum, x, y, ylim, sumsq_ptr, sum_ptr
  %define wm     [esp+0]
  %define hm     [esp+4]
  %define edgem  [esp+8]
@@ -2127,7 +2127,7 @@ cglobal sgr_box5_v, 5, 7, 8, -44, sumsq, sum, x, y, ylim, sumsq_ptr, sum_ptr
     jmp .sum_loop_y_noload
 %endif
 
-cglobal sgr_calc_ab2, 4, 7, 11, a, b, w, h, s
+cglobal sgr_calc_ab2_8bpc, 4, 7, 11, a, b, w, h, s
     movifnidn     sd, sm
     sub           aq, (384+16-1)*4
     sub           bq, (384+16-1)*2
@@ -2205,7 +2205,7 @@ cglobal sgr_calc_ab2, 4, 7, 11, a, b, w, h, s
     RET
 
 %if ARCH_X86_64
-cglobal sgr_finish_filter2, 5, 13, 14, t, src, stride, a, b, w, h, \
+cglobal sgr_finish_filter2_8bpc, 5, 13, 14, t, src, stride, a, b, w, h, \
                                        tmp_base, src_base, a_base, b_base, x, y
     movifnidn     wd, wm
     mov           hd, hm
@@ -2219,7 +2219,7 @@ cglobal sgr_finish_filter2, 5, 13, 14, t, src, stride, a, b, w, h, \
     psrlw        m11, m12, 1                    ; pw_128
     pxor         m13, m13
 %else
-cglobal sgr_finish_filter2, 6, 7, 8, t, src, stride, a, b, x, y
+cglobal sgr_finish_filter2_8bpc, 6, 7, 8, t, src, stride, a, b, x, y
  %define tmp_baseq  r0m
  %define src_baseq  r1m
  %define a_baseq    r3m
@@ -2378,7 +2378,7 @@ cglobal sgr_finish_filter2, 6, 7, 8, t, src, stride, a, b, x, y
     RET
 
 %undef t2
-cglobal sgr_weighted2, 4, 7, 12, dst, stride, t1, t2, w, h, wt
+cglobal sgr_weighted2_8bpc, 4, 7, 12, dst, stride, t1, t2, w, h, wt
     movifnidn     wd, wm
     movd          m0, wtm
 %if ARCH_X86_64
-- 
GitLab

