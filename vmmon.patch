diff --git a/vmmon/Makefile b/vmmon/Makefile
index de8162e..6124a71 100644
--- a/vmmon/Makefile
+++ b/vmmon/Makefile
@@ -43,7 +43,11 @@ INCLUDE      += -I$(SRCROOT)/shared
 endif
 
 
+ifdef KVERSION
+VM_UNAME = $(KVERSION)
+else
 VM_UNAME = $(shell uname -r)
+endif
 
 # Header directory for the running kernel
 ifdef LINUXINCLUDE
diff --git a/vmmon/Makefile.kernel b/vmmon/Makefile.kernel
index bf805e0..9aac585 100644
--- a/vmmon/Makefile.kernel
+++ b/vmmon/Makefile.kernel
@@ -22,7 +22,7 @@ CC_OPTS += -DVMMON -DVMCORE
 INCLUDE := -I$(SRCROOT)/include -I$(SRCROOT)/common -I$(SRCROOT)/linux \
 	   -I$(SRCROOT)/vmcore
 
-EXTRA_CFLAGS := $(CC_OPTS) $(INCLUDE)
+EXTRA_CFLAGS := $(CC_OPTS) $(INCLUDE) $(LINUXINCLUDE)
 
 EXTRA_CFLAGS += $(call vm_check_build, $(SRCROOT)/autoconf/smpcall.c, -DVMW_HAVE_SMP_CALL_3ARG, )
 EXTRA_CFLAGS += $(call vm_check_build, $(SRCROOT)/autoconf/tsc_khz.c, -DVMW_HAVE_TSC_KHZ, )
diff --git a/vmmon/include/compat_module.h b/vmmon/include/compat_module.h
index 2af7372..729aedc 100644
--- a/vmmon/include/compat_module.h
+++ b/vmmon/include/compat_module.h
@@ -80,4 +80,13 @@ static const char __module_cat(tag, __LINE__)[]                               \
    typedef int compat_mod_param_bool;
 #endif
 
+/*
+ * Linux kernel >= 4.2.99 does not return anything from misc_deregister
+ */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 2, 99)
+#define compat_misc_deregister(misc) misc_deregister(misc)
+#else
+#define compat_misc_deregister(misc) ({misc_deregister(misc);0;})
+#endif
+
 #endif /* __COMPAT_MODULE_H__ */
diff --git a/vmmon/linux/driver.c b/vmmon/linux/driver.c
index 87cf45b..5390a93 100644
--- a/vmmon/linux/driver.c
+++ b/vmmon/linux/driver.c
@@ -126,7 +126,16 @@ static struct vm_operations_struct vmuser_mops = {
 #endif
 };
 
-static struct file_operations vmuser_fops;
+static struct file_operations vmuser_fops = {
+   .owner = THIS_MODULE,
+   .poll = LinuxDriverPoll,
+   .unlocked_ioctl = LinuxDriver_Ioctl,
+   .compat_ioctl = LinuxDriver_Ioctl,
+   .open = LinuxDriver_Open,
+   .release = LinuxDriver_Close,
+   .mmap = LinuxDriverMmap
+};
+
 static struct timer_list tscTimer;
 static Atomic_uint32 tsckHz;
 static VmTimeStart tsckHzStartTime;
@@ -256,7 +265,7 @@ static void
 LinuxDriverInitTSCkHz(void)
 {
    unsigned int khz;
- 
+
    khz = compat_tsc_khz();
    if (khz != 0) {
       Atomic_Write(&tsckHz, khz);
@@ -271,7 +280,7 @@ LinuxDriverInitTSCkHz(void)
    add_timer(&tscTimer);
 }
 
- 
+
 /*
  *----------------------------------------------------------------------
  *
@@ -319,21 +328,6 @@ init_module(void)
    linuxState.fastClockPriority = -20;
    linuxState.swapSize = VMMON_UNKNOWN_SWAP_SIZE;
 
-   /*
-    * Initialize the file_operations structure. Because this code is always
-    * compiled as a module, this is fine to do it here and not in a static
-    * initializer.
-    */
-
-   memset(&vmuser_fops, 0, sizeof vmuser_fops);
-   vmuser_fops.owner = THIS_MODULE;
-   vmuser_fops.poll = LinuxDriverPoll;
-   vmuser_fops.unlocked_ioctl = LinuxDriver_Ioctl;
-   vmuser_fops.compat_ioctl = LinuxDriver_Ioctl;
-   vmuser_fops.open = LinuxDriver_Open;
-   vmuser_fops.release = LinuxDriver_Close;
-   vmuser_fops.mmap = LinuxDriverMmap;
-
 #ifdef VMX86_DEVEL
    devel_init_module();
    linuxState.minor = 0;
@@ -389,7 +383,9 @@ cleanup_module(void)
 #ifdef VMX86_DEVEL
    unregister_chrdev(linuxState.major, linuxState.deviceName);
 #else
-   misc_deregister(&linuxState.misc);
+   if (compat_misc_deregister(&linuxState.misc)) {
+      Warning("Module %s: error unregistering\n", linuxState.deviceName);
+   }
 #endif
 
    Log("Module %s: unloaded\n", linuxState.deviceName);
@@ -1287,7 +1283,7 @@ LinuxDriverReadTSC(void *data,   // OUT: TSC values
  *-----------------------------------------------------------------------------
  */
 
-__attribute__((always_inline)) static Bool
+inline __attribute__((always_inline)) static Bool
 LinuxDriverSyncReadTSCs(uint64 *delta) // OUT: TSC max - TSC min
 {
    TSCDelta tscDelta;
@@ -1976,7 +1972,7 @@ LinuxDriver_Ioctl(struct file *filp,    // IN:
       break;
    }
 
-   default: 
+   default:
       Warning("Unknown ioctl %d\n", iocmd);
       retval = -EINVAL;
    }
diff --git a/vmmon/linux/hostif.c b/vmmon/linux/hostif.c
index fd32013..583d6da 100644
--- a/vmmon/linux/hostif.c
+++ b/vmmon/linux/hostif.c
@@ -37,6 +37,9 @@
 
 #include <linux/vmalloc.h>
 #include <linux/slab.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 99)
+#include <linux/sched/signal.h>
+#endif
 
 #include <linux/preempt.h>
 #include <linux/poll.h>
@@ -73,6 +76,7 @@
 #include <linux/capability.h>
 #include <linux/kthread.h>
 #include <linux/wait.h>
+#include <asm/apic.h>
 #include <linux/signal.h>
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 11, 0)
 #include <linux/taskstats_kern.h> // For linux/sched/signal.h without version check
@@ -99,6 +99,37 @@
 #include "vmmonInt.h"
 #include "versioned_atomic.h"
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
+#   define global_zone_page_state global_page_state
+#endif
+
+static unsigned long get_nr_slab_unreclaimable(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 13, 0)
+   return global_node_page_state(NR_SLAB_UNRECLAIMABLE);
+#else
+   return global_page_state(NR_SLAB_UNRECLAIMABLE);
+#endif
+}
+
+static unsigned long get_nr_unevictable(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 8, 0)
+   return global_node_page_state(NR_UNEVICTABLE);
+#else
+   return global_page_state(NR_UNEVICTABLE);
+#endif
+}
+
+static unsigned long get_nr_anon_mapped(void)
+{
+ #if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 8, 0)
+   return global_node_page_state(NR_ANON_MAPPED);
+ #else
+   return global_page_state(NR_ANON_PAGES);
+ #endif
+}
+
 /*
  * Determine if we can use high resolution timers.
  */
@@ -142,6 +146,10 @@
  */
 #define LOCKED_PAGE_SLACK 10000
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(4, 7, 99)
+#define NR_ANON_PAGES NR_ANON_MAPPED
+#endif
+
 static struct {
    Atomic_uint64     uptimeBase;
    VersionedAtomic   version;
@@ -438,7 +446,7 @@
  * HostIF_WakeUpYielders --
  *
  *      Wakeup vCPUs that are waiting for the current vCPU.
- *      
+ *
  * Results:
  *      The requested vCPUs are nudged if they are sleeping due to
  *      Vmx86_YieldToSet.
@@ -521,7 +529,7 @@
  *      None
  *
  * Side effects:
- *      Should be a very low contention lock. 
+ *      Should be a very low contention lock.
  *      The current thread is rescheduled if the lock is busy.
  *
  *-----------------------------------------------------------------------------
@@ -564,7 +572,7 @@
  * HostIF_GlobalLockIsHeld --
  *
  *      Determine if the global lock is held by the current thread.
- * 
+ *
  * Results:
  *      TRUE if yes
  *      FALSE if no
@@ -594,7 +602,7 @@
  *      None
  *
  * Side effects:
- *      Should be a very low contention lock. 
+ *      Should be a very low contention lock.
  *      The current thread is rescheduled if the lock is busy.
  *
  *-----------------------------------------------------------------------------
@@ -729,7 +737,7 @@
 HostIFHostMemInit(VMDriver *vm)  // IN:
 {
    VMHost *vmh = vm->vmhost;
-   
+
    vmh->lockedPages = PhysTrack_Alloc(vm);
    if (!vmh->lockedPages) {
       return -1;
@@ -824,8 +832,8 @@
  *
  * HostIF_FreeMachinePage --
  *
- *      Free an anonymous machine page allocated by 
- *      HostIF_AllocMachinePage().  This page is not tracked in any 
+ *      Free an anonymous machine page allocated by
+ *      HostIF_AllocMachinePage().  This page is not tracked in any
  *      phystracker.
  *
  * Results:
@@ -1118,7 +1126,7 @@
  * Results:
  *      prevents INTR #0x2d (IRQ 13) from being generated --
  *      assume that Int16 works for interrupt reporting
- *      
+ *
  *
  * Side effects:
  *      PIC
@@ -1133,7 +1141,7 @@
 
    uint8 val = inb(0xA1);
 
-   if (!(val & mask)) { 
+   if (!(val & mask)) {
       val = val | mask;
       outb(val, 0xA1);
    }
@@ -1149,7 +1157,7 @@
  *      If ppages is NULL, pages are only marked as dirty.
  *
  * Results:
- *      Zero on success, non-zero on failure. 
+ *      Zero on success, non-zero on failure.
  *
  * Side effects:
  *      None
@@ -1165,14 +1173,16 @@
    int retval;
 
    down_read(&current->mm->mmap_sem);
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 9, 0)
-   retval = get_user_pages((unsigned long)uvAddr, numPages, 0, ppages, NULL);
-#elif LINUX_VERSION_CODE >= KERNEL_VERSION(4, 6, 0)
-   retval = get_user_pages((unsigned long)uvAddr, numPages, 0, 0, ppages, NULL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 5, 99)
+   retval = get_user_pages((unsigned long)uvAddr,
 #else
    retval = get_user_pages(current, current->mm, (unsigned long)uvAddr,
-                           numPages, 0, 0, ppages, NULL);
 #endif
+                           numPages, 0,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+                           0,
+#endif
+                           ppages, NULL);
    up_read(&current->mm->mmap_sem);
 
    return retval != numPages;
@@ -1184,11 +1194,11 @@
  *
  * HostIF_IsLockedByMPN --
  *
- *      Checks if mpn was locked using allowMultipleMPNsPerVA.  
+ *      Checks if mpn was locked using allowMultipleMPNsPerVA.
  *
  * Results:
  *      TRUE if mpn is present in the physTracker.
- *      
+ *
  *
  * Side effects:
  *     None.
@@ -1310,7 +1320,7 @@
 
    vpn = VA_2_VPN((VA)addr);
    e = MemTrack_LookupVPN(vm->memtracker, vpn);
-    
+
    if (e == NULL) {
       return PAGE_UNLOCK_NOT_TRACKED;
    }
@@ -1356,7 +1366,7 @@
    {
       void *va = VA64ToPtr(uAddr);
       MemTrackEntry *e;
-      
+
       /*
        * Verify for debugging that VA and MPN make sense.
        * PgtblVa2MPN() can fail under high memory pressure.
@@ -1374,7 +1384,7 @@
       }
 
       /*
-       * Verify that this MPN was locked with 
+       * Verify that this MPN was locked with
        * HostIF_LockPage(allowMultipleMPNsPerVA = TRUE).
        * That means that this MPN should not be in the MemTracker.
        */
@@ -1387,7 +1397,7 @@
          return PAGE_UNLOCK_MISMATCHED_TYPE;
       }
    }
-#endif 
+#endif
 
    HOST_UNLOCK_PFN_BYMPN(vm, mpn);
 
@@ -1395,7 +1405,7 @@
 }
 
 
-static void 
+static void
 UnlockEntry(void *clientData,         // IN:
             MemTrackEntry *entryPtr)  // IN:
 {
@@ -1458,11 +1468,11 @@
  *
  * HostIF_AllocKernelMem
  *
- *      Allocate some kernel memory for the driver. 
+ *      Allocate some kernel memory for the driver.
  *
  * Results:
- *      The address allocated or NULL on error. 
- *      
+ *      The address allocated or NULL on error.
+ *
  *
  * Side effects:
  *      memory is malloced
@@ -1474,8 +1484,8 @@
                       int wired)    // IN:
 {
    void * ptr = kmalloc(size, GFP_KERNEL);
-   
-   if (ptr == NULL) { 
+
+   if (ptr == NULL) {
       Warning("%s failed (size=%p)\n", __func__, (void*)size);
    }
 
@@ -1503,7 +1513,7 @@
 HostIF_AllocPage(void)
 {
    VA kvAddr;
-   
+
    kvAddr = __get_free_page(GFP_KERNEL);
    if (kvAddr == 0) {
       Warning("%s: __get_free_page() failed\n", __func__);
@@ -1518,7 +1528,7 @@
  *
  * HostIF_FreeKernelMem
  *
- *      Free kernel memory allocated for the driver. 
+ *      Free kernel memory allocated for the driver.
  *
  * Results:
  *      None.
@@ -1557,7 +1567,7 @@
  *      from the kernel without causing the host to die or to be really upset.
  *
  * Results:
- *	The maximum number of pages that can be locked. 
+ *	The maximum number of pages that can be locked.
  *
  * Side effects:
  *      none
@@ -1585,9 +1595,9 @@
     * Use the memory information linux exports as of late for a more
     * precise estimate of locked memory.  All kernel page-related structures
     * (slab, pagetable) are as good as locked.  Unevictable includes things
-    * that are explicitly marked as such (like mlock()).  Huge pages are 
-    * also as good as locked, since we don't use them.  Lastly, without 
-    * available swap, anonymous pages become locked in memory as well. 
+    * that are explicitly marked as such (like mlock()).  Huge pages are
+    * also as good as locked, since we don't use them.  Lastly, without
+    * available swap, anonymous pages become locked in memory as well.
     */
 
    unsigned int forHost;
@@ -1594,16 +1625,11 @@ HostIF_EstimateLockedPageLimit(const VMDriver* vm,                // IN
    unsigned int reservedPages = MEMDEFAULTS_MIN_HOST_PAGES;
    unsigned int hugePages = (vm == NULL) ? 0 :
       BYTES_2_PAGES(vm->memInfo.hugePageBytes);
-   unsigned int lockedPages = global_page_state(NR_PAGETABLE) +
-                              global_page_state(NR_SLAB_UNRECLAIMABLE) +
-                              global_page_state(NR_UNEVICTABLE) +
+   unsigned int lockedPages = global_zone_page_state(NR_PAGETABLE) +
+                              get_nr_slab_unreclaimable() +
+                              get_nr_unevictable() +
                               hugePages + reservedPages;
-   unsigned int anonPages =
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 8, 0)
-      global_page_state(NR_ANON_MAPPED);
-#else
-      global_page_state(NR_ANON_PAGES);
-#endif
+   unsigned int anonPages = get_nr_anon_mapped();
    unsigned int swapPages = BYTES_2_PAGES(linuxState.swapSize);
 
    if (anonPages > swapPages) {
@@ -1607,7 +1617,7 @@
    unsigned int swapPages = BYTES_2_PAGES(linuxState.swapSize);
 
    if (anonPages > swapPages) {
-      lockedPages += anonPages - swapPages; 
+      lockedPages += anonPages - swapPages;
    }
    forHost = lockedPages + LOCKED_PAGE_SLACK;
    if (forHost > totalPhysicalPages) {
@@ -1652,7 +1662,7 @@
  *----------------------------------------------------------------------
  */
 
-void 
+void
 HostIF_WaitForFreePages(unsigned int timeoutMs)  // IN:
 {
    static unsigned count;
@@ -1677,20 +1687,20 @@
  *      timeofday to have small drift (due to NTP rate correction, etc).
  *      We handle this by rebasing the jiffies based monotonic clock
  *      every second (see HostIFUptimeResyncMono).
- *      
+ *
  * Results:
  *      The uptime, in units of UPTIME_FREQ.  Also returns the jiffies
  *      value that was used in the monotonic time calculation.
  *
  * Side effects:
- *      May reset the uptime base in the case gettimeofday warp was 
+ *      May reset the uptime base in the case gettimeofday warp was
  *      detected.
  *
  *----------------------------------------------------------------------
  */
 
 static uint64
-HostIFReadUptimeWork(unsigned long *j)  // OUT: current jiffies 
+HostIFReadUptimeWork(unsigned long *j)  // OUT: current jiffies
 {
    struct timeval tv;
    uint64 monotime, uptime, upBase, monoBase;
@@ -1712,14 +1722,14 @@
 
    do_gettimeofday(&tv);
    upBase = Atomic_Read64(&uptimeState.uptimeBase);
-   
+
    monotime = (uint64)(jifs - jifBase) * (UPTIME_FREQ / HZ);
    monotime += monoBase;
 
    uptime = tv.tv_usec * (UPTIME_FREQ / 1000000) + tv.tv_sec * UPTIME_FREQ;
    uptime += upBase;
-   
-   /* 
+
+   /*
     * Use the jiffies based monotonic time to sanity check gettimeofday.
     * If they differ by more than one second, assume the time of day has
     * been warped, and use the jiffies time to undo (most of) the warp.
@@ -1731,7 +1741,7 @@
       uint64 newUpBase = monotime - (uptime - upBase);
 
       attempts++;
-      if (!Atomic_CMPXCHG64(&uptimeState.uptimeBase, &upBase, &newUpBase) && 
+      if (!Atomic_CMPXCHG64(&uptimeState.uptimeBase, &upBase, &newUpBase) &&
           attempts < 5) {
          /* Another thread updated uptimeBase.  Recalculate uptime. */
          goto retry;
@@ -1772,7 +1782,7 @@
    unsigned long jifs;
    uintptr_t flags;
 
-   /* 
+   /*
     * Read the uptime and the corresponding jiffies value.  This will
     * also correct the uptime (which is based on time of day) if needed
     * before we rebase monotonic time (which is based on jiffies).
@@ -1780,7 +1790,7 @@
 
    uint64 uptime = HostIFReadUptimeWork(&jifs);
 
-   /* 
+   /*
     * Every second, recalculate monoBase and jiffiesBase to squash small
     * drift between gettimeofday and jiffies.  Also, this prevents
     * (jiffies - jiffiesBase) wrap on 32-bits.
@@ -1825,8 +1835,8 @@
 
    uptimeState.jiffiesBase = jiffies;
    do_gettimeofday(&tv);
-   Atomic_Write64(&uptimeState.uptimeBase, 
-                  -(tv.tv_usec * (UPTIME_FREQ / 1000000) + 
+   Atomic_Write64(&uptimeState.uptimeBase,
+                  -(tv.tv_usec * (UPTIME_FREQ / 1000000) +
                     tv.tv_sec * UPTIME_FREQ));
 
    init_timer(&uptimeState.timer);
@@ -1954,7 +1964,7 @@
  *-----------------------------------------------------------------------------
  */
 
-int 
+int
 HostIF_CopyToUser(void *dst,        // OUT
                   const void *src,  // IN
                   unsigned int len) // IN
@@ -1967,15 +1977,15 @@
  *-----------------------------------------------------------------------------
  *
  * HostIF_MapCrossPage --
- *    
- *    Obtain kernel pointer to crosspage. 
  *
- *    We must return a VA that is obtained through a kernel mapping, so that 
+ *    Obtain kernel pointer to crosspage.
+ *
+ *    We must return a VA that is obtained through a kernel mapping, so that
  *    the mapping never goes away (see bug 29753).
  *
- *    However, the LA corresponding to that VA must not overlap with the 
- *    monitor (see bug 32922). The userland code ensures that by only 
- *    allocating cross pages from low memory. For those pages, the kernel 
+ *    However, the LA corresponding to that VA must not overlap with the
+ *    monitor (see bug 32922). The userland code ensures that by only
+ *    allocating cross pages from low memory. For those pages, the kernel
  *    uses a permanent mapping, instead of a temporary one with a high LA.
  *
  * Results:
@@ -2163,7 +2173,7 @@
  *      None
  *
  * Side effects:
- *      Can wake up the thread blocked on this lock. 
+ *      Can wake up the thread blocked on this lock.
  *
  *-----------------------------------------------------------------------------
  */
@@ -2186,7 +2196,7 @@
  * HostIF_VMLockIsHeld --
  *
  *      Determine if the per-VM lock is held by the current thread.
- * 
+ *
  * Results:
  *      TRUE if yes
  *      FALSE if no
@@ -2242,14 +2252,14 @@
  *
  *----------------------------------------------------------------------
  */
- 
+
 static Bool
 isVAReadable(VA r)  // IN:
 {
    mm_segment_t old_fs;
    uint32 dummy;
    int ret;
-   
+
    old_fs = get_fs();
    set_fs(get_ds());
    r = APICR_TO_ADDR(r, APICR_VERSION);
@@ -2324,7 +2334,7 @@
 	  Bool setVMPtr)  // IN: set a pointer to the APIC's virtual address
 {
    MA ma = APIC_GetMA();
-   
+
    if (ma == (MA)-1) {
       return FALSE;
    }
@@ -2425,7 +2435,7 @@
  *    Perform the semaphore wait (P) operation, possibly blocking.
  *
  * Result:
- *    1 (which equals MX_WAITNORMAL) if success, 
+ *    1 (which equals MX_WAITNORMAL) if success,
  *    negated error code otherwise.
  *
  * Side-effects:
@@ -2434,7 +2444,7 @@
  *-----------------------------------------------------------------------------
  */
 
-int   
+int
 HostIF_SemaphoreWait(VMDriver *vm,   // IN:
                      Vcpuid vcpuid,  // IN:
                      uint64 *args)   // IN:
@@ -2457,7 +2467,7 @@
    {
       struct poll_wqueues table;
       unsigned int mask;
-      
+
       poll_initwait(&table);
       current->state = TASK_INTERRUPTIBLE;
       mask = file->f_op->poll(file, &table.pt);
@@ -2532,7 +2542,7 @@
  *-----------------------------------------------------------------------------
  */
 
-void 
+void
 HostIF_SemaphoreForceWakeup(VMDriver *vm,       // IN:
                             const VCPUSet *vcs) // IN:
 {
@@ -2920,7 +2930,7 @@
    if (ptr == NULL) {
       return -ENOMEM;
    }
-   
+
    if (kernelBuffer) {
       memcpy(buf, ptr, PAGE_SIZE);
    } else {
@@ -3106,7 +3116,7 @@
  * Side effects:
  *    None.
  *
- *---------------------------------------------------------------------- 
+ *----------------------------------------------------------------------
  */
 
 uint32
@@ -3124,18 +3134,18 @@
  *
  *      Wake up the fast clock thread.  Can't do this from the timer
  *      callback, because it holds locks that the scheduling code
- *      might take. 
+ *      might take.
  *
  * Results:
  *      None.
- *      
+ *
  * Side effects:
  *      None.
  *
  *----------------------------------------------------------------------
  */
 
-static void 
+static void
 HostIFWakeupClockThread(unsigned long data)  //IN:
 {
    wake_up_process(linuxState.fastClockThread);
@@ -3146,7 +3156,7 @@
  *----------------------------------------------------------------------
  *
  * HostIFTimerCallback --
- *      
+ *
  *      Schedule a tasklet to wake up the fast clock thread.
  *
  * Results:
@@ -3157,8 +3167,8 @@
  *
  *----------------------------------------------------------------------
  */
- 
-static enum hrtimer_restart 
+
+static enum hrtimer_restart
 HostIFTimerCallback(struct hrtimer *timer)  //IN:
 {
    tasklet_schedule(&timerTasklet);
@@ -3171,7 +3181,7 @@
  *----------------------------------------------------------------------
  *
  * HostIFScheduleHRTimeout --
- *      
+ *
  *      Schedule an hrtimer to wake up the fast clock thread.
  *
  * Results:
@@ -3183,7 +3193,7 @@
  *----------------------------------------------------------------------
  */
 
-static void 
+static void
 HostIFScheduleHRTimeout(ktime_t *expires)  //IN:
 {
    struct hrtimer t;
@@ -3201,7 +3211,7 @@
    if (hrtimer_active(&t)) {
       schedule();
    }
-   
+
    hrtimer_cancel(&t);
    __set_current_state(TASK_RUNNING);
 }
@@ -3224,7 +3234,7 @@
  * Side effects:
  *    none.
  *
- *---------------------------------------------------------------------- 
+ *----------------------------------------------------------------------
  */
 
 static long
@@ -3257,7 +3267,7 @@
  */
 
 int
-HostIFStartTimer(Bool rateChanged,  //IN: Did rate change? 
+HostIFStartTimer(Bool rateChanged,  //IN: Did rate change?
 		 unsigned int rate, //IN: current clock rate
                  struct file *filp) //IN: /dev/rtc descriptor
 {
@@ -3267,14 +3277,14 @@
    int timerPeriod;
 
    if (rateChanged) {
-      timerPeriod = NSEC_PER_SEC / rate; 
+      timerPeriod = NSEC_PER_SEC / rate;
       expires = ktime_set(0, timerPeriod);
       /*
        * Allow the kernel to expire the timer at its convenience.
        * ppoll() uses 0.1% of the timeout value.  I think we can
        * tolerate 1%.
        */
-          
+
       slack = timerPeriod / 100;
    }
    set_current_state(TASK_INTERRUPTIBLE);
@@ -3620,7 +3630,7 @@
 
    printk(KERN_DEBUG "%s: p = 0x%p, offset = 0x%p, numPagesNeeded = %"FMTSZ"u,"
           " handleSize = %"FMTSZ"u, mappedAddr = 0x%p\n",
-          __func__, p, (void *)offset, numPagesNeeded, handleSize, mappedAddr); 
+          __func__, p, (void *)offset, numPagesNeeded, handleSize, mappedAddr);
 
    newHandle->numPages = numPagesNeeded;
    newHandle->addr = mappedAddr;
@@ -3656,7 +3666,7 @@
    }
 
    printk(KERN_DEBUG "%s: numPages = %"FMTSZ"u, addr = 0x%p\n",
-          __func__, handle->numPages, handle->addr); 
+          __func__, handle->numPages, handle->addr);
 
    if (handle->numPages > 1) {
       vunmap(handle->addr);
diff --git a/vmmon/linux/vmmonInt.h b/vmmon/linux/vmmonInt.h
index 4dc04e9..993212d 100644
--- a/vmmon/linux/vmmonInt.h
+++ b/vmmon/linux/vmmonInt.h
@@ -28,7 +28,7 @@
  * Hide all kernel compatibility stuff in these macros and functions.
  */
 
-#ifdef VMW_HAVE_SMP_CALL_3ARG
+#if defined (VMW_HAVE_SMP_CALL_3ARG) || LINUX_VERSION_CODE >= KERNEL_VERSION(3, 0, 0)
 #define compat_smp_call_function(fn, info, wait) smp_call_function(fn, info, wait)
 #else
 #define compat_smp_call_function(fn, info, wait) smp_call_function(fn, info, 1, wait)
