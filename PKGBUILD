# Maintainer: Chris Severance aur.severach AatT spamgourmet.com
# Contributor: David Roheim <david dot roheim at gmail dot com>
# Contributor: Manuel Hoffmann <manuel@manuel-hoffmann.info>
# Contributor: Markus Holtermann <aur@markusholtermann.eu>
# Contributor: Mantas Vidutis <mantas.a.vidutis-at-gmail.com>
# Contributor: Tianjiao Yin <ytj000@gmail.com>

set -u
pkgname='hadoop'
pkgver='3.2.1'
pkgrel='1'
pkgdesc='Hadoop - MapReduce implementation and distributed filesystem'
arch=('i686' 'x86_64')
url='http://hadoop.apache.org'
license=('apache')
depends=('java-environment>=7' 'openssh' 'apache-ant' 'polkit')
backup=(
  "etc/conf.d/${pkgname}"
  "etc/profile.d/${pkgname}.sh"
  "etc/${pkgname}/capacity-scheduler.xml"
  "etc/${pkgname}/configuration.xsl"
  "etc/${pkgname}/core-site.xml"
  "etc/${pkgname}/fair-scheduler.xml"
  "etc/${pkgname}/hadoop-env.sh"
  "etc/${pkgname}/hadoop-metrics2.properties"
  "etc/${pkgname}/hadoop-policy.xml"
  "etc/${pkgname}/hdfs-site.xml"
  "etc/${pkgname}/log4j.properties"
  "etc/${pkgname}/mapred-queue-acls.xml"
  "etc/${pkgname}/mapred-site.xml"
  "etc/${pkgname}/masters"
  "etc/${pkgname}/slaves"
  "etc/${pkgname}/ssl-client.xml.example"
  "etc/${pkgname}/ssl-server.xml.example"
  "etc/${pkgname}/taskcontroller.cfg"
  "etc/${pkgname}/task-log4j.properties"
)
options=('!strip')
install="${pkgname}.install"
source=(
  "http://mirror.reverse.net/pub/apache/hadoop/common/hadoop-${pkgver}/hadoop-${pkgver}.tar.gz"
  'hadoop.profile'
  'hadoop-conf'
  'hadoop.sh'
  'hadoop-namenode.service'
  'hadoop-datanode.service'
  'hadoop-secondarynamenode.service'
  'hadoop-jobtracker.service'
  'hadoop-tasktracker.service'
)
_verwatch=("${source[0]%|*}hadoop/common/" '.*href="hadoop-\([0-9\.]\+\)/.*' 'f')
md5sums=('489ceea09aa6c78bde5c2241f9da742d'
         '77fad322bff1877b0c5b4e6d693c979a'
         '2b662c5d0548cae29538060bbea9e96b'
         '56a70b8c94de7c1fb236ec24d7c11e05'
         '4e96bfa974fb7701b5636379c02f8470'
         '287feaae2d479042d7210ea5ef079a5e'
         '4dc609ae8d536dbb278a7e89c523384f'
         'dba52a72c925365bc50a2e443a38f7f4'
         '8da68ae4b6f20a969df19945d359fc32')
sha256sums=('f66a3a4115b8f16c1077d1a198a06854dbef0e4233291712ed08d0a10629ed37'
            'b6607cb8531244d9be9241d8d4980d5695869f89fa598c8d24b35ec503df413b'
            'e584c32246fd23fe5f35b13399372419584c27a234364b12d1479f3c87e70748'
            '93cb40f76f6bb0c1924b7ef083b82d39bf32190f86c28fc6304839703cdda7b1'
            '3fd40045f7657881cde0abee4ac1735232ba3d79121d724f74707252e19088b3'
            '230a58ab4e3462eb63662aee057965c5130247f7d9c98df83495c8da2c409fe5'
            '047d3d6aea9eada82780eaa93a55c6259fb1b63c68bc50cc26323e066c1b7f75'
            '5e9bc41b0086dfa7b237d1a7248a7f113299687f79ba0c58ba01eaeea0e35e79'
            '37d7a252292b365782d9e7a64d6019a78d9c561acf9b5af3c246b602d3e0a8ec')
PKGEXT='.pkg.tar.gz' # Not worth the extra time to save 10%, not compatible with pacaur

compile() {
  set -u
  cd "${pkgname}-${pkgver}"
  msg 'Cleaning...'
  ant clean

  msg 'Patching...'
  sed -i -e "s/${_devver}/${pkgver}/" 'build.xml'
  sed -i -e "s|<ivysettings>|<ivysettings>\n<caches defaultCacheDir=\"${srcdir}/ivy_cache\"/>|" 'ivy/ivysettings.xml'

  msg "Building..."
  ant -D'compile.native=true' bin-package
  set +u
}

package() {
  set -u
  local _usr_lib="${pkgdir}/usr/lib"
  local _hadoop_real_home="${_usr_lib}/${pkgname}-${pkgver}"
  local _hadoop_link_home="${_usr_lib}/${pkgname}"

  install -d "${_usr_lib}" "${pkgdir}/usr/lib/systemd/system"
  cp -pr "${srcdir}/${pkgname}-${pkgver}" "${_usr_lib}"

  #
  install -Dpm755 "${srcdir}/hadoop-conf" "${pkgdir}/etc/conf.d/hadoop"
  install -Dpm755 "${srcdir}/hadoop.profile" "${pkgdir}/etc/profile.d/hadoop.sh"
  install -Dpm644 "${srcdir}/"hadoop-*.service -t "${pkgdir}/usr/lib/systemd/system/"

  # we do not use soft link because we need put configures in backup array,
  # in order to preserve the conf when upgrade package.
  cp -pr "${_hadoop_real_home}/etc/hadoop" "${pkgdir}/etc"
  mv "${_hadoop_real_home}/etc" "${_hadoop_real_home}/orig_etc"

  # todo: i need an own file :)
  install -Dm755 <(cat << EOF
#!/bin/sh
# Automatically generated by ${pkgname}-${pkgver} PKGBUILD from Arch Linux AUR
# https://aur.archlinux.org/
for f in /etc/profile.d/*.sh; do
  . "\${f}"
done
/usr/lib/hadoop/bin/hadoop "\$@"
EOF
  ) "${pkgdir}/usr/bin/hadoop"

  cd "${_usr_lib}"
  ln -s "${pkgname}-${pkgver}" "${pkgname}"

  ## Disable IPv6 (comment out to disable IPv6 support):
  # sed -i 's|_OPTS="-D|_OPTS="-Djava.net.preferIPv4Stack=true -D|' hadoop-env.sh

if ! :; then
  ## fix native
  if [ "${CARCH}" = 'i686' ]; then
    rm -rf 'lib/native/Linux-amd64-64'
    cd 'lib/native/Linux-i386-32'
    sed -i -e "s:dependency_libs=':dependency_libs='-L/opt/java/jre/lib/i386/server :" 'libhadoop.la'
  fi

  if [ "${CARCH}" = 'x86_64' ]; then
    rm -rf 'lib/native/Linux-i386-32'
    cd 'lib/native/Linux-amd64-64'
    sed -i "s:dependency_libs=':dependency_libs='-L/opt/java/jre/lib/amd64/server :" 'libhadoop.la'
  fi

  ## Create some links, so Hadoop's KFS jar could access KFS libraries properly
  ## (it is still fine if KFS is not installed)

  msg 'Creating KFS links...'

  for _lib in 'libkfsClient' 'libkfsCommon' 'libkfsEmulator' 'libkfsIO' 'libkfsMeta'; do
    for _ext in 'a' 'so'; do
      ln -sf "/usr/lib/${_lib}.${_ext}"
    done
  done
  ln -sf '/usr/lib/libkfs_access.so'
fi
  set +u

}
set +u
